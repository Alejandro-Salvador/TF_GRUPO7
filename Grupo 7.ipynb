{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO FINAL\n",
    "Lectura de genes de la pagina **UNIPROT**\n",
    "\n",
    "https://www.uniprot.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUPO 7:\n",
    " - Salvador Cama, Alejandro 20190310\n",
    " - Velasquez Gonzales, Pedro Daniel 20170382\n",
    " - Farfán Florián Erik David 20160229\n",
    " - Inoñan Sandoval, José Agustín  20190299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genename = []                                                                # Creamos una lista vacia llamada genename\n",
    "for i in range(18):                                                          # Para i en una rango de (0 a 17)\n",
    "    if i == 0:                                                               # Si i es igual a 0   \n",
    "        data = pd.read_excel(\"02 Ch Genes ordenados con vecinos.xlsx\",       # Nombre de la data\n",
    "                    sheet_name=i,                                            # Numero de hoja, en este caso sera 0\n",
    "                     index_col=\"Gen Abrev\",                                  # La columna que sera el index\n",
    "                    skiprows=0)                                     \n",
    "    else:\n",
    "        data = pd.read_excel(\"02 Ch Genes ordenados con vecinos.xlsx\",      # Nombre de la data\n",
    "                        sheet_name=i,                                       # Numero de la hoja\n",
    "                         index_col=\"Gen Abrev\",                             # La columna que sera el index\n",
    "                        skiprows=0)                                         # saltar la primera fila\n",
    "    data.to_csv(\"genes.csv\")                                                # Convertimos la data en un csv\n",
    "    \n",
    "    ef = pd.read_csv('genes.csv')              # leemos la data y lo asignamos a la variable ef\n",
    "    genename.extend(list(ef[\"Gen Abrev\"]))     # Agregamos la columna Gen Abrev a la lista genename\n",
    "    \n",
    "pagina=[]                                                                           # Creamos una lista vacia llamada pagina, para guadar todas las paginas\n",
    "for i in genename:                                                                  # i tomara cada elemento de genename\n",
    "    url = \"https://www.uniprot.org/uniprot/?query=\"+str(i)+\"+Capra+Hircus&sort=score\" # Concatenamos cadenas para las paginas\n",
    "    pagina.append(url)                                                              # cada pagina lo vamos a apendisar a la lista pagina\n",
    "print(genename[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos listas vacías de cada columna del excel para luego ir rellenadolo\n",
    "entry = []                                                                          # Creamos una lista varia llamada entry \n",
    "entry_name=[]                                                                       # Creamos una lista varia llamada entry_name \n",
    "protein =[]                                                                         # Creamos una lista varia llamada protein\n",
    "gene =[]                                                                            # Creamos una lista varia llamada gene\n",
    "organism =[]                                                                        # Creamos una lista varia llamada organism \n",
    "status =[]                                                                          # Creamos una lista varia llamada status\n",
    "Go_Biologicalprocess = [] \n",
    "\n",
    "for i in pagina:                                            # i tomara cada elemento de la lista pagina\n",
    "    html = requests.get(i)                                  # hacemos una peticion para obtener el contenido de la web\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')       # Extraemos la informacion en formato html\n",
    "    etiquetas = soup.find('tr',class_=\"entry selected-row\") # de lo extraido vamo a encontrar la etiqueta con nombre 'tr' y clase \"entry selected-row\"\n",
    "    if etiquetas == None:                                   # Si etiquetas es vacio\n",
    "        entry.append(\"Null\")                                  # La lista entry le agregamos un na\n",
    "        entry_name.append(\"Null\")                             # La lista entry_name le agregamos un na\n",
    "        protein.append(\"Null\")                                # La lista Protein le agregamos un na\n",
    "        gene.append(\"Null\")                                   # La lista gene le agregamos un na\n",
    "        organism.append(\"Null\")                               # La lista organism le agregamos un na\n",
    "        status.append(\"Null\")                                 # La lista status le agregamos un na\n",
    "        Go_Biologicalprocess.append(\"Null\")                      # La lista Biologicalprocess le agregamos un na\n",
    "                                   \n",
    "    else:                                                   # Si etiquetas no es vacio\n",
    "        a =etiquetas.find(\"td\",class_=\"entryID\").text       # de lo extraido vamos a encontrar la etiqueta con nombre 'td' y clase \"entryID\" y solo obtendremos el texto y asignamo a la variable a\n",
    "        entry.append(a)                                     # el string \"a\" le adjuntamos a la lista entry\n",
    "        entry_name.append(etiquetas.findAll(\"td\")[2].text)  # de etiquetas vamos a obtener el texto de la segunda etiqueta \"td\" y lo juntamos a la lista entry_name\n",
    "        url1 = \"https://www.uniprot.org/uniprot/\"+str(a)    # Concatenamos para las paginas web y le asignamos a la variable url1\n",
    "        html1 = requests.get(url1)                          # hacemos una peticion para obtener el contenido de la web\n",
    "        soup1 = BeautifulSoup(html1.content, 'html.parser') # Extraemos la informacion en formato html y le asignamos a la variable soup1\n",
    "         \n",
    "        etiquetas1 = soup1.find(\"div\", id=\"entry-overview\") # de lo extraido de soup1 vamos a encontrar la etiqueta con nombre 'div' y id \"entry-overview\" y asignamo a la variable \"etiquetas1\" \n",
    "        \n",
    "        pro= etiquetas1.find(\"h1\", property=\"name\")\n",
    "        if pro != None:                                     # Si pro no es vacio\n",
    "            protein.append(pro.text)                        # Le agregamos el texto  a la lista protein\n",
    "        else:                                               # Caso contario\n",
    "            protein.append(\"Null\")                            # le agregamos \"na\"\n",
    "            \n",
    "        ge = etiquetas1.find(\"div\", id=\"content-gene\")      # De lo extraido en etiquetas1 vamos a encontrar las etiquetas con nombre \"div\" y id \"content-gene\" asignandolo a ge\n",
    "        if ge != None:                                      # Si \"ge\" no es vacio\n",
    "            gene.append(ge.text)                            # Le agregamos el texto de ge a la lista gene\n",
    "        else:                                               # caso contrario\n",
    "            gene.append(\"Null\")                               # Le agregamos \"na\"\n",
    "            \n",
    "        orga= etiquetas1.find(\"div\", id=\"content-organism\") # De lo extraido en etiquetas1 vamos a encontrar las etiquetas con nombre \"div\" y id \"content-organism\" asignandolo a orga\n",
    "        if orga != None:                                    # si orga no es vacio\n",
    "            organism.append(orga.text)                      # Le agregamos el texto de orga a la lista organism\n",
    "        else:                                               # Caso contrario\n",
    "            organism.append(\"Null\")                           # Le agregamos \"na\"\n",
    "            \n",
    "        sta = etiquetas1.find(\"a\", href=\"/manual/entry_status\") # De lo extraido en etiquetas1 vamos a encontrar las etiquetas con nombre \"a\" y href debe de contener \"/manual/entry_status\" asignandolo a sta\n",
    "        if sta != None:                                         # Si sta no es vacio\n",
    "            status.append(sta.text)                             # Le agregamos el texto de sta a la lista status\n",
    "        else:                                                   \n",
    "            status.append(\"Null\")\n",
    "        \n",
    "        function = soup1.find(\"div\", id=\"function\")                                           # De lo extraido en soup vamos a encontrar las etiquetas \"div\" y div \"function\" para asignarle a una variable function\n",
    "        if function != None:                                                                  # Si function no es vacio\n",
    "            etiquetas2 = function.find(\"ul\", class_=\"noNumbering biological_process\")         # De lo extraido en function vamos a buscar las etiquetas \"ul\" y la clase \"noNumbering biological_process\" asignandole a la variable etiquetas2\n",
    "\n",
    "            if etiquetas2 != None:                                                            # Si etiquetas2 no es vacio\n",
    "                biological = etiquetas2.findAll(\"a\",href=re.compile(\"https://www.ebi.ac.uk\")) # De etiquetas dos vamos a encontrar todos las etiquetas \"a\" y los href que tengan \"https://www.ebi.ac.uk\"\n",
    "\n",
    "                h=[]                           # Creamos una lista vacia con nombre h\n",
    "                for i in biological:           # i tomara cada elemento de biological\n",
    "                    h.append(i.text)           # Cada elemento de biological obtenemos su texto y le agregamos a la lista h\n",
    "                h = str(\". \".join(h))          # Concatenamos cadenas de los elementos de la lista para que solo haya un elemento en la lista y le asignamos a h\n",
    "                Go_Biologicalprocess.append(h) # Recien agregamos h a la lista Biologicalprocess\n",
    "            else:                              \n",
    "                Go_Biologicalprocess.append(\"Null\")  # Agregamos \"Null\"\n",
    "        else:\n",
    "            Go_Biologicalprocess.append(\"Null\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos la longitudes para corroborrar que todos tienen la misma longitud para hace un  dataFrame               \n",
    "print(len(entry))\n",
    "print(len(entry_name))\n",
    "print(len(protein))\n",
    "print(len(gene))\n",
    "print(len(organism))\n",
    "print(len(status))\n",
    "print(len(Go_Biologicalprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario con todas las listas creadas y sus llaves el nombre que sera para las columnas\n",
    "diccionario={\"GENEABREV\":genename,\"ENTRY\":entry,\"ENTRY NAME\":entry_name, \"PROTEIN\":protein,\n",
    "            \"GENE\":gene,\"ORGANISM\":organism,\"STATUS\":status, \"GO - BIOLOGICAL PROCESS\":Go_Biologicalprocess\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
